{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 11533909,
          "sourceType": "datasetVersion",
          "datasetId": 7223205
        }
      ],
      "dockerImageVersionId": 31012,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import copy\n",
        "from collections import deque\n",
        "import time"
      ],
      "metadata": {
        "id": "NX1K_bRqsjiN"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Device setup and transforms\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(p=0.1),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.RandomAffine(degrees=5, translate=(0.05, 0.05), scale=(0.95, 1.05)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3hzfKOZkMj0",
        "outputId": "006e0a7e-ca13-43fe-fba6-b7d5ad252572"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "execution_count": 16
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4OyMj0KkTVz",
        "outputId": "4d5060b5-6e10-40e4-e31b-99e5c4de1b7b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Dataset class\n",
        "class RunwayImageDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels=None, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        label = self.labels[idx] if self.labels else -1\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "4wUSCWXOygmi",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 17
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Class definitions and mappings\n",
        "GRF_CLASSES = {\n",
        "    \"Dry\": 6,\n",
        "    \"Damp\": 5,\n",
        "    \"Compacted Snow\": 4,\n",
        "    \"Wet\": 3,\n",
        "    \"Standing Water\": 2,\n",
        "    \"Ice\": 1,\n",
        "    \"Wet Ice\": 0\n",
        "}\n",
        "\n",
        "GRF_CLASS_NAMES = {v: k for k, v in GRF_CLASSES.items()}\n",
        "\n",
        "folder_to_grf = {\n",
        "    \"asphalt_dry\": 6, \"concrete_dry\": 6, \"cobble_dry\": 6,\n",
        "    \"asphalt_damp\": 5, \"concrete_damp\": 5, \"cobble_damp\": 5,\n",
        "    \"fully_packed\": 4,\n",
        "    \"asphalt_wet\": 3, \"concrete_wet\": 3, \"cobble_wet\": 3,\n",
        "    \"asphalt_verywet\": 2, \"concrete_verywet\": 2, \"cobble_verywet\": 2,\n",
        "    \"fresh_snow\": 1, \"fresh_fallen\": 1,\n",
        "    \"partially_covered\": 0\n",
        "}"
      ],
      "metadata": {
        "id": "robOXwjPyiQE"
      },
      "outputs": [],
      "execution_count": 18
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Dataset preparation function\n",
        "def prepare_datasets(base_dirs, transform_fn):\n",
        "    image_paths, labels = [], []\n",
        "\n",
        "    class_counts = {label: 0 for label in range(7)}\n",
        "\n",
        "    for base_dir in base_dirs:\n",
        "        if not os.path.exists(base_dir):\n",
        "            print(f\"Warning: Directory {base_dir} does not exist. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        for folder in os.listdir(base_dir):\n",
        "            class_dir = os.path.join(base_dir, folder)\n",
        "            if os.path.isdir(class_dir) and folder in folder_to_grf:\n",
        "                grf_label = folder_to_grf[folder]\n",
        "                files = [f for f in os.listdir(class_dir) if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
        "\n",
        "                for file in files:\n",
        "                    image_paths.append(os.path.join(class_dir, file))\n",
        "                    labels.append(grf_label)\n",
        "                    class_counts[grf_label] += 1\n",
        "\n",
        "    if not image_paths:\n",
        "        raise ValueError(f\"No images found in the specified directories: {base_dirs}\")\n",
        "\n",
        "    print(\"Class distribution:\")\n",
        "    for label, count in class_counts.items():\n",
        "        if count > 0:\n",
        "            print(f\"  {GRF_CLASS_NAMES[label]} (RWYCC {label}): {count} images\")\n",
        "\n",
        "    total = sum(class_counts.values())\n",
        "    print(f\"Total: {total} images\")\n",
        "\n",
        "    return RunwayImageDataset(image_paths, labels, transform_fn)"
      ],
      "metadata": {
        "id": "T2bjUr_KD0UI",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T13:05:08.780127Z",
          "iopub.execute_input": "2025-05-05T13:05:08.780450Z",
          "iopub.status.idle": "2025-05-05T13:05:08.787086Z",
          "shell.execute_reply.started": "2025-05-05T13:05:08.780424Z",
          "shell.execute_reply": "2025-05-05T13:05:08.785955Z"
        }
      },
      "outputs": [],
      "execution_count": 19
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Model definition\n",
        "class GRFClassifier(nn.Module):\n",
        "    def __init__(self, backbone='efficientnet_b3', num_classes=7):\n",
        "        super(GRFClassifier, self).__init__()\n",
        "\n",
        "        if backbone == 'mobilenetv2':\n",
        "            self.model = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)\n",
        "            in_features = self.model.classifier[1].in_features\n",
        "            self.model.classifier[1] = nn.Linear(in_features, num_classes)\n",
        "        else:\n",
        "            self.model = models.efficientnet_b3(weights=models.EfficientNet_B3_Weights.DEFAULT)\n",
        "            in_features = self.model.classifier[1].in_features\n",
        "            self.model.classifier[1] = nn.Linear(in_features, num_classes)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.model.classifier[1].weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "bXC81OTHD9wC",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-24T12:33:12.516839Z",
          "iopub.execute_input": "2025-04-24T12:33:12.517598Z",
          "iopub.status.idle": "2025-04-24T12:33:12.522873Z",
          "shell.execute_reply.started": "2025-04-24T12:33:12.517565Z",
          "shell.execute_reply": "2025-04-24T12:33:12.521976Z"
        }
      },
      "outputs": [],
      "execution_count": 20
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Evaluation function\n",
        "def evaluate_model(model, dataloader, device, class_names=None):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_confidences = []\n",
        "    inference_times = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            start_time = time.time()\n",
        "            outputs = model(inputs)\n",
        "            inference_time = time.time() - start_time\n",
        "\n",
        "            probs = torch.nn.functional.softmax(outputs, dim=1)\n",
        "            confidences, preds = torch.max(probs, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_confidences.extend(confidences.cpu().numpy())\n",
        "            inference_times.append(inference_time / inputs.size(0))\n",
        "\n",
        "    avg_inference_time = np.mean(inference_times)\n",
        "    fps = 1.0 / avg_inference_time if avg_inference_time > 0 else 0\n",
        "\n",
        "    print(f\"Average inference time: {avg_inference_time*1000:.2f} ms per image\")\n",
        "    print(f\"Estimated throughput: {fps:.2f} FPS\")\n",
        "\n",
        "    print(\"\\nClassification Report:\")\n",
        "    if class_names:\n",
        "        report = classification_report(all_labels, all_preds, target_names=class_names, output_dict=True)\n",
        "        print(classification_report(all_labels, all_preds, target_names=class_names))\n",
        "    else:\n",
        "        report = classification_report(all_labels, all_preds, output_dict=True)\n",
        "        print(classification_report(all_labels, all_preds))\n",
        "\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    if class_names:\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "    else:\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    correct = np.array(all_preds) == np.array(all_labels)\n",
        "    sns.histplot(\n",
        "        x=all_confidences,\n",
        "        hue=correct,\n",
        "        element=\"step\",\n",
        "        stat=\"density\",\n",
        "        common_norm=False,\n",
        "        bins=30,\n",
        "        palette=[\"red\", \"green\"]\n",
        "    )\n",
        "    plt.title(\"Confidence Distribution (Correct vs. Incorrect Predictions)\")\n",
        "    plt.xlabel(\"Confidence\")\n",
        "    plt.ylabel(\"Density\")\n",
        "    plt.legend([\"Incorrect\", \"Correct\"])\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    eval_results = {\n",
        "        'predictions': all_preds,\n",
        "        'labels': all_labels,\n",
        "        'confidences': all_confidences,\n",
        "        'inference_time': avg_inference_time,\n",
        "        'fps': fps,\n",
        "        'report': report,\n",
        "        'confusion_matrix': cm\n",
        "    }\n",
        "\n",
        "    return eval_results"
      ],
      "metadata": {
        "id": "iWeWfYf7ABvN",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-24T18:52:25.753917Z",
          "iopub.execute_input": "2025-04-24T18:52:25.754448Z",
          "iopub.status.idle": "2025-04-24T18:52:25.834443Z",
          "shell.execute_reply.started": "2025-04-24T18:52:25.754427Z",
          "shell.execute_reply": "2025-04-24T18:52:25.833915Z"
        }
      },
      "outputs": [],
      "execution_count": 12
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Training function\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=20, patience=5):\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    trigger_times = 0\n",
        "\n",
        "    history = {\n",
        "        'train_loss': [], 'train_acc': [],\n",
        "        'val_loss': [], 'val_acc': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        model.train()\n",
        "        running_loss, running_correct, total = 0.0, 0, 0\n",
        "        epoch_start_time = time.time()\n",
        "\n",
        "        print(f\"Epoch {epoch}/{num_epochs} - Training Phase\")\n",
        "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, preds = outputs.max(1)\n",
        "            running_correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            if batch_idx % 10 == 0:\n",
        "                print(f\"  Batch {batch_idx}/{len(train_loader)} - Loss: {loss.item():.4f}\")\n",
        "\n",
        "        epoch_time = time.time() - epoch_start_time\n",
        "        train_loss = running_loss / total\n",
        "        train_acc = running_correct / total\n",
        "        print(f\"[Train] Epoch {epoch} Summary: Loss {train_loss:.4f}, Accuracy {train_acc:.4f}, Time {epoch_time:.2f}s\")\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item() * images.size(0)\n",
        "                _, preds = outputs.max(1)\n",
        "                val_correct += (preds == labels).sum().item()\n",
        "                val_total += labels.size(0)\n",
        "\n",
        "        val_loss = val_loss / val_total\n",
        "        val_acc = val_correct / val_total\n",
        "        print(f\"[Validation] Epoch {epoch} Summary: Loss {val_loss:.4f}, Accuracy {val_acc:.4f}\")\n",
        "\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            trigger_times = 0\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_loss': val_loss,\n",
        "                'val_acc': val_acc,\n",
        "                'train_loss': train_loss,\n",
        "                'train_acc': train_acc,\n",
        "            }, \"best_model_checkpoint.pth\")\n",
        "            print(\"  ✅ Checkpoint saved (best model so far)\")\n",
        "        else:\n",
        "            trigger_times += 1\n",
        "            print(f\"  ⚠️ No improvement for {trigger_times} epoch(s)\")\n",
        "            if trigger_times >= patience:\n",
        "                print(\"⛔ Early stopping triggered\")\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history['train_loss'], label='Train Loss')\n",
        "    plt.plot(history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Loss vs. Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history['train_acc'], label='Train Accuracy')\n",
        "    plt.plot(history['val_acc'], label='Validation Accuracy')\n",
        "    plt.title('Accuracy vs. Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return model, history"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-24T18:52:36.782938Z",
          "iopub.execute_input": "2025-04-24T18:52:36.783203Z",
          "iopub.status.idle": "2025-04-24T18:52:36.787690Z",
          "shell.execute_reply.started": "2025-04-24T18:52:36.783186Z",
          "shell.execute_reply": "2025-04-24T18:52:36.787006Z"
        },
        "id": "EllP_f8fkMj4"
      },
      "outputs": [],
      "execution_count": 21
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Runway condition monitor\n",
        "class RunwayConditionMonitor:\n",
        "    def __init__(self, model, window_size=30, threshold=0.6):\n",
        "        self.model = model\n",
        "        self.model.eval()\n",
        "        self.window_size = window_size\n",
        "        self.threshold = threshold\n",
        "        self.scores_history = deque(maxlen=window_size)\n",
        "        self.transform = val_transform\n",
        "\n",
        "    def process_frame(self, frame):\n",
        "        if isinstance(frame, np.ndarray):\n",
        "            frame = Image.fromarray(frame)\n",
        "\n",
        "        img_tensor = self.transform(frame).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = self.model(img_tensor)\n",
        "            probs = torch.nn.functional.softmax(output, dim=1)[0]\n",
        "            confidence, pred_class = torch.max(probs, dim=0)\n",
        "\n",
        "        self.scores_history.append(probs.cpu().numpy())\n",
        "\n",
        "        result = {\n",
        "            'rwycc': int(pred_class.item()),\n",
        "            'condition': GRF_CLASS_NAMES[pred_class.item()],\n",
        "            'confidence': float(confidence.item()),\n",
        "            'probabilities': {GRF_CLASS_NAMES[i]: float(prob) for i, prob in enumerate(probs.cpu().numpy())}\n",
        "        }\n",
        "\n",
        "        if len(self.scores_history) >= self.window_size:\n",
        "            trend = self.analyze_trend()\n",
        "            result['trend'] = trend\n",
        "\n",
        "        return result\n",
        "\n",
        "    def analyze_trend(self):\n",
        "        score_history = np.array(self.scores_history)\n",
        "\n",
        "        window_size = min(5, len(score_history))\n",
        "        smoothed_scores = np.zeros_like(score_history)\n",
        "        for i in range(score_history.shape[1]):\n",
        "            smoothed_scores[:, i] = np.convolve(score_history[:, i],\n",
        "                                              np.ones(window_size)/window_size,\n",
        "                                              mode='same')\n",
        "\n",
        "        recent_window = max(5, len(smoothed_scores) // 2)\n",
        "        recent_scores = smoothed_scores[-recent_window:]\n",
        "\n",
        "        x = np.arange(recent_window).reshape(-1, 1)\n",
        "        slopes = np.zeros(score_history.shape[1])\n",
        "\n",
        "        for i in range(score_history.shape[1]):\n",
        "            y = recent_scores[:, i]\n",
        "            mean_x, mean_y = np.mean(x), np.mean(y)\n",
        "            slopes[i] = np.sum((x - mean_x) * (y - mean_y)) / np.sum((x - mean_x) ** 2)\n",
        "\n",
        "        current_class = np.argmax(score_history[-1])\n",
        "        current_slope = slopes[current_class]\n",
        "\n",
        "        if current_slope > 0.01:\n",
        "            return \"improving\"\n",
        "        elif current_slope < -0.01:\n",
        "            return \"worsening\"\n",
        "        else:\n",
        "            return \"stable\"\n",
        "\n",
        "    def generate_grf_report(self, location, runway_id):\n",
        "        if not self.scores_history:\n",
        "            return {\"error\": \"No frames processed yet\"}\n",
        "\n",
        "        latest = self.process_frame(None)\n",
        "        timestamp = time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime())\n",
        "\n",
        "        grf_report = {\n",
        "            \"timestamp\": timestamp,\n",
        "            \"location\": location,\n",
        "            \"runway\": runway_id,\n",
        "            \"rwycc\": latest['rwycc'],\n",
        "            \"surface_condition\": latest['condition'],\n",
        "            \"confidence\": latest['confidence'],\n",
        "            \"trend\": latest.get('trend', \"unknown\")\n",
        "        }\n",
        "\n",
        "        return grf_report"
      ],
      "metadata": {
        "id": "G9-PkCHZEjwK",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-24T12:33:39.411570Z",
          "iopub.execute_input": "2025-04-24T12:33:39.412188Z",
          "iopub.status.idle": "2025-04-24T12:33:39.417262Z",
          "shell.execute_reply.started": "2025-04-24T12:33:39.412161Z",
          "shell.execute_reply": "2025-04-24T12:33:39.416509Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your dataset paths here\n",
        "train_dirs = [\n",
        "    \"/content/drive/MyDrive/datasets/RoadSaW-075_s/train\",\n",
        "    \"/content/drive/MyDrive/datasets/RoadSC/train\"\n",
        "]\n",
        "\n",
        "val_dirs = [\n",
        "    \"/content/drive/MyDrive/datasets/RoadSaW-075_s/validation\",\n",
        "    \"/content/drive/MyDrive/datasets/RoadSC/validation\"\n",
        "]\n",
        "\n",
        "test_dirs = [\n",
        "    \"/content/drive/MyDrive/datasets/RoadSaW-075_s/test\",\n",
        "    \"/content/drive/MyDrive/datasets/RoadSC/test\"\n",
        "]\n",
        "\n",
        "# Prepare datasets\n",
        "train_dataset = prepare_datasets(train_dirs, transform)\n",
        "val_dataset = prepare_datasets(val_dirs, val_transform)\n",
        "test_dataset = prepare_datasets(test_dirs, val_transform)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "\n",
        "# Initialize model\n",
        "model = GRFClassifier(backbone='efficientnet_b3', num_classes=7).to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
        "\n",
        "# Train model\n",
        "model, history = train_model(\n",
        "    model, train_loader, val_loader,\n",
        "    criterion, optimizer, scheduler,\n",
        "    num_epochs=30, patience=5\n",
        ")\n",
        "\n",
        "# Class names for evaluation\n",
        "class_names = [GRF_CLASS_NAMES[i] for i in range(7)]\n",
        "\n",
        "# Evaluate on test set\n",
        "test_results = evaluate_model(model, test_loader, device, class_names)\n",
        "\n",
        "# Save the final model\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'class_mapping': folder_to_grf,\n",
        "    'grf_class_names': GRF_CLASS_NAMES,\n",
        "    'performance': {\n",
        "        'test_accuracy': test_results['report']['accuracy'],\n",
        "        'f1_macro': test_results['report']['macro avg']['f1-score'],\n",
        "        'inference_time_ms': test_results['inference_time'] * 1000,\n",
        "        'fps': test_results['fps']\n",
        "    }\n",
        "}, 'runway_condition_classifier_final.pth')"
      ],
      "metadata": {
        "id": "2OwhA-hOtr1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize monitor with trained model\n",
        "runway_monitor = RunwayConditionMonitor(model, window_size=30)\n",
        "\n",
        "# Simulating processing of video frames\n",
        "import cv2\n",
        "cap = cv2.VideoCapture('/content/drive/MyDrive/datasetsrunway_video.mp4')\n",
        "\n",
        "results = []\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    result = runway_monitor.process_frame(frame_rgb)\n",
        "    results.append(result)\n",
        "\n",
        "    condition_text = f\"{result['condition']} (RWYCC: {result['rwycc']}, Conf: {result['confidence']:.2f})\"\n",
        "    cv2.putText(frame, condition_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "\n",
        "    if 'trend' in result:\n",
        "        trend_text = f\"Trend: {result['trend']}\"\n",
        "        cv2.putText(frame, trend_text, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "\n",
        "    cv2.imshow('Runway Condition Monitor', frame)\n",
        "\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "# Generate GRF report\n",
        "grf_report = runway_monitor.generate_grf_report(\"EGLL\", \"27L\")\n",
        "print(grf_report)\n"
      ],
      "metadata": {
        "id": "wM6uaS5btvqG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}